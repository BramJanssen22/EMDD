{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance measures\n",
    "\n",
    "def prediction_measures(y_test,y_pred):\n",
    "    performance = classification_report(y_test,y_pred,output_dict=True)\n",
    "    return performance['depression'],performance['normal'],performance['accuracy']\n",
    "\n",
    "# # Support measures\n",
    "\n",
    "# def percentage_depression(pred):\n",
    "#     return np.sum(pred == 'depression') / len(pred)\n",
    "\n",
    "# def true_positives(pred, true):\n",
    "#     return np.sum((pred == 'depression') & (true == \"depression\"))\n",
    "\n",
    "# def true_negatives(pred, true):\n",
    "#     return np.sum((pred == 'normal') & (true == \"normal\"))\n",
    "\n",
    "# def true_positive_rate(pred, true):\n",
    "#     return true_positives(pred,true)/(true_positives(pred,true)+true_negatives(pred,true))\n",
    "\n",
    "# def true_negative_rate(pred, true):\n",
    "#     return true_negatives(pred,true)/(true_positives(pred,true)+true_negatives(pred,true))\n",
    "\n",
    "# def calculate_rates(pred, true):\n",
    "#     tp = np.sum((pred == 'depression') & (true == 'depression'))\n",
    "#     tn = np.sum((pred == 'normal') & (true == 'normal'))\n",
    "#     fn = np.sum((pred == 'normal') & (true == 'depression'))\n",
    "#     fp = np.sum((pred == 'depression') & (true == 'normal'))\n",
    "#     tpr = tp / (tp + fn) if (tp + fn) != 0 else 0  # True Positive Rate\n",
    "#     tnr = tn / (tn + fp) if (tn + fp) != 0 else 0  # True Negative Rate\n",
    "#     return tpr, tnr, tp, tn, fp, fn\n",
    "\n",
    "# # Complete measures\n",
    "\n",
    "# def statistical_parity(predsensitive,predother):\n",
    "#     return percentage_depression(predsensitive)/percentage_depression(predother)\n",
    "\n",
    "# def equal_opportunity(predsensitive, truesensitive, predother, trueother):\n",
    "#     return true_positive_rate(predsensitive,truesensitive)/true_positive_rate(predother,trueother)\n",
    "\n",
    "# def equalised_odds(predsensitive, truesensitive, predother, trueother):\n",
    "#     return (true_positive_rate(predsensitive,truesensitive)+true_negative_rate(predsensitive,truesensitive))/(true_positive_rate(predother,trueother)+true_negative_rate(predsensitive,truesensitive))\n",
    "\n",
    "# def equal_accuracy(predsensitive, truesensitive, predother, trueother):\n",
    "#     return accuracy_score(predsensitive,truesensitive)/accuracy_score(predother,trueother)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All measurements\n",
    "\n",
    "def all_measures(predsensitive, truesensitive, predother, trueother, name='test',single=False):\n",
    "    score_dict = {}\n",
    "    score_dict['predictor'] = name\n",
    "    # Metrics for sensitive group\n",
    "    depression_performance, normal_performance ,accuracy = prediction_measures(predsensitive,truesensitive)\n",
    "    score_dict['depressionSensPrecision'] = depression_performance['precision']\n",
    "    score_dict['depressionSensRecall'] = depression_performance['recall']\n",
    "    score_dict['depressionSensF1'] = depression_performance['f1-score']\n",
    "    score_dict['depressionSensSupport'] = depression_performance['support']\n",
    "    score_dict['normalSensPrecision'] = normal_performance['precision']\n",
    "    score_dict['normalSensRecall'] = normal_performance['recall']\n",
    "    score_dict['normalSensF1'] = normal_performance['f1-score']\n",
    "    score_dict['normalSensSupport'] = normal_performance['support']\n",
    "    score_dict['accuracySens'] = accuracy\n",
    "    if single == False:\n",
    "        # Metrics for other group\n",
    "        depression_performance, normal_performance ,accuracy = prediction_measures(predother,trueother)\n",
    "        score_dict['depressionOtherPrecision'] = depression_performance['precision']\n",
    "        score_dict['depressionOtherRecall'] = depression_performance['recall']\n",
    "        score_dict['depressionOtherF1'] = depression_performance['f1-score']\n",
    "        score_dict['depressionOtherSupport'] = depression_performance['support']\n",
    "        score_dict['normalOtherPrecision'] = normal_performance['precision']\n",
    "        score_dict['normalOtherRecall'] = normal_performance['recall']\n",
    "        score_dict['normalOtherF1'] = normal_performance['f1-score']\n",
    "        score_dict['normalOtherSupport'] = normal_performance['recall']\n",
    "        score_dict['accuracyOther'] = accuracy\n",
    "        # Metrics for total\n",
    "        depression_performance, normal_performance ,accuracy = prediction_measures(pd.concat([predsensitive,predother]),pd.concat([truesensitive,trueother]))\n",
    "        score_dict['depressionTotalPrecision'] = depression_performance['precision']\n",
    "        score_dict['depressionTotalRecall'] = depression_performance['recall']\n",
    "        score_dict['depressionTotalF1'] = depression_performance['f1-score']\n",
    "        score_dict['depressionTotalSupport'] = depression_performance['support']\n",
    "        score_dict['normalTotalPrecision'] = normal_performance['precision']\n",
    "        score_dict['normalTotalRecall'] = normal_performance['recall']\n",
    "        score_dict['normalTotalF1'] = normal_performance['f1-score']\n",
    "        score_dict['normalTotalSupport'] = normal_performance['recall']\n",
    "        score_dict['accuracyTotal'] = accuracy\n",
    "        # Fairness metrics between groups\n",
    "        TNS, FPS, FNS, TPS = confusion_matrix(truesensitive, predsensitive).ravel()\n",
    "        TNO, FPO, FNO, TPO = confusion_matrix(trueother,predother).ravel()\n",
    "        score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
    "        score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n",
    "        score_dict['equalisedOdds'] =  ((TPS / (TPS+FNS)) + (TNS / (TNS+FPS))) / ((TPO / (TPO+FNO))+ (TNO) / (TNO+FPO))\n",
    "        score_dict['equalAccuracy'] = (accuracy_score(truesensitive,predsensitive)/accuracy_score(trueother,predother))\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:44: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:44: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:44: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:44: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/bram/anaconda3/envs/OZP/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:43: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['statisticalParity'] = ((TPS+FPS)/len(truesensitive)) / ((TPO+FPO)/len(trueother))\n",
      "/var/folders/qw/2ytfcz3d78qfsgsvq73r4dx80000gn/T/ipykernel_35561/1924135758.py:44: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  score_dict['equalOpportunity'] = (TPS / (TPS+FNS)) / (TPO / (TPO+FNO))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "options = ['w2vec','ruleBased','DictionaryBased','TFIDF','blingfire','reweightedblingfire','nltk','reweightednltk','spacysm','reweightedspacysm','spacylg','reweightedspacylg','spacytrf','reweightedspacytrf']\n",
    "\n",
    "for name in options:\n",
    "    df = pd.read_json(f'data/predictionData/{name}Pred.json', lines=True, orient='records')\n",
    "\n",
    "    pred_male = df.loc[(df['gender'] == 'm') & (df['split'] == 'test'), ['prediction']]\n",
    "    pred_female = df.loc[(df['gender'] == 'f') & (df['split'] == 'test'), ['prediction']]\n",
    "    y_testm = df.loc[(df['gender'] == 'm') & (df['split'] == 'test'), ['label']]\n",
    "    y_testf = df.loc[(df['gender'] == 'f') & (df['split'] == 'test'), ['label']]\n",
    "\n",
    "    score_dict = all_measures(pred_female,y_testf,pred_male,y_testm, name)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv('data/results/testresults.csv')\n",
    "    except:\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "    df_new = pd.DataFrame(score_dict,index=[len(df)])\n",
    "\n",
    "    df = pd.concat([df,df_new],ignore_index=True)\n",
    "\n",
    "    df.to_csv('data/results/testresults.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OZP-compatibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
