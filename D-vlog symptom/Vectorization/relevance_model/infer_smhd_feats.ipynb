{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import yaml\n",
    "import xopen\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from os.path import dirname\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from data import infer_preprocess, cut_sentences\n",
    "from model import Classifier, BERTDiseaseClassifier\n",
    "from utils import default_symps\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         True\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.5\n",
      "\"exp_name\":           mbert_label_enhance_bal_sample_050_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data_w_control\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          exclude\n",
      "\"write_result_dir\":   ./lightning_logs/bal_sample_records.json\n"
     ]
    }
   ],
   "source": [
    "datastore = []\n",
    "df = pd.read_json(\"../../postdatalines.json\", lines=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 64\n",
    "    input_dir = \"../../postdatalines.json\"\n",
    "    ckpt_dir = \"lightning_logs/version_0/checkpoints/epoch=0-step=720.ckpt\"\n",
    "    hparams_dir = os.path.join(dirname(dirname(ckpt_dir)), 'hparams.yaml')\n",
    "    hparams = yaml.load(open(hparams_dir),Loader=yaml.Loader)\n",
    "    max_len = hparams[\"max_len\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hparams[\"model_type\"])\n",
    "    clf = Classifier.load_from_checkpoint(ckpt_dir, symps=default_symps)\n",
    "    clf.eval()\n",
    "    clf.cuda()\n",
    "    \n",
    "    with xopen.xopen(input_dir) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            record = json.loads(line)\n",
    "            user_sents = []\n",
    "            sent_bounds = [0]\n",
    "            curr_sid = 0\n",
    "            if record['text'] == None:\n",
    "                break\n",
    "            else:\n",
    "                for post in record[\"text\"]:\n",
    "                    sents = cut_sentences(post)\n",
    "                    curr_sid += len(sents)\n",
    "                    sent_bounds.append(curr_sid)\n",
    "                    user_sents.extend(sents)\n",
    "                all_probs = []\n",
    "                all_feats = []\n",
    "                for i in range(0, len(user_sents), batch_size):\n",
    "                    curr_texts = user_sents[i:i+batch_size]\n",
    "                    processed_batch = infer_preprocess(curr_texts, tokenizer, max_len)\n",
    "                    for k, v in processed_batch.items():\n",
    "                        processed_batch[k] = v.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        feats, logits = clf.feat_extract_avg(processed_batch)\n",
    "                        feats = feats.detach().cpu().numpy()\n",
    "                        probs = logits.sigmoid().detach().cpu().numpy()\n",
    "                    all_probs.append(probs)\n",
    "                    all_feats.append(feats)\n",
    "                all_probs = np.concatenate(all_probs, 0)\n",
    "                all_feats = np.concatenate(all_feats, 0)\n",
    "\n",
    "                # merge all sentence features into post-level feature by max pooling\n",
    "                all_post_probs = []\n",
    "                all_post_feats = []\n",
    "                for i in range(len(sent_bounds)-1):\n",
    "                    lbound, rbound = sent_bounds[i], sent_bounds[i+1]\n",
    "                    post_prob = all_probs[lbound:rbound, :].max(0)\n",
    "                    all_post_probs.append(post_prob)\n",
    "                    post_feat = all_feats[lbound:rbound, :].mean(0)\n",
    "                    all_post_feats.append(post_feat)\n",
    "                all_post_probs = np.stack(all_post_probs, 0)\n",
    "                all_post_feats = np.stack(all_post_feats, 0)\n",
    "                data = (dict(zip(range(38),all_post_probs[0])))\n",
    "                datastore.append(data)\n",
    "\n",
    "df = pd.read_json(\"../../postdatalines.json\", lines=True)\n",
    "vector_df = pd.DataFrame(datastore)\n",
    "df = pd.concat([df,vector_df],axis=1)\n",
    "df.to_json(\"../postdatalinesvectors.json\",lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OZP-compatibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
