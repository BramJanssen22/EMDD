{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import yaml\n",
    "import xopen\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from os.path import dirname\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from data import infer_preprocess\n",
    "from model import Classifier, BERTDiseaseClassifier\n",
    "from utils import decide_subject\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import blingfire\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentences(text, tokenizer, nlp):\n",
    "    if tokenizer == 'blingfire':\n",
    "        sents = blingfire.text_to_sentences(text.strip()).split(\"\\n\")\n",
    "    if tokenizer == 'nltk':\n",
    "        sents = sent_tokenize(text.strip())\n",
    "    if tokenizer == 'spacysm':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if tokenizer == 'spacylg':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if tokenizer == 'spacytrf':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         False\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.75\n",
      "\"exp_name\":           mbert_uncertain_only_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          only\n",
      "\"write_result_dir\":   ./lightning_logs/baseline_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [00:22, 35.48it/s]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         False\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.75\n",
      "\"exp_name\":           mbert_uncertain_only_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          only\n",
      "\"write_result_dir\":   ./lightning_logs/baseline_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [00:19, 40.76it/s]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         False\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.75\n",
      "\"exp_name\":           mbert_uncertain_only_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          only\n",
      "\"write_result_dir\":   ./lightning_logs/baseline_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [03:00,  4.42it/s]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         False\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.75\n",
      "\"exp_name\":           mbert_uncertain_only_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          only\n",
      "\"write_result_dir\":   ./lightning_logs/baseline_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [03:13,  4.12it/s]\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         False\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.75\n",
      "\"exp_name\":           mbert_uncertain_only_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          only\n",
      "\"write_result_dir\":   ./lightning_logs/baseline_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [26:30,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "options = ['blingfire', 'nltk', 'spacysm', 'spacylg', 'spacytrf']\n",
    "for name in options:\n",
    "    datastore = []\n",
    "    senttokenizer = name\n",
    "    nlp = ''\n",
    "    # set spacy tokenizer\n",
    "    if senttokenizer == 'spacysm':\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if senttokenizer == 'spacylg':\n",
    "        nlp = spacy.load(\"en_core_web_lg\")\n",
    "    if senttokenizer == 'spacytrf':\n",
    "        nlp = spacy.load(\"en_core_web_trf\")    \n",
    "\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        batch_size = 64\n",
    "        input_dir = \"../../../data/postdatalines.json\"\n",
    "        ckpt_dir = \"lightning_logs/version_0/checkpoints/epoch=1-step=133.ckpt\"\n",
    "        hparams_dir = os.path.join(dirname(dirname(ckpt_dir)), 'hparams.yaml')\n",
    "        hparams = yaml.load(open(hparams_dir),Loader=yaml.Loader)\n",
    "        max_len = hparams[\"max_len\"]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hparams[\"model_type\"])\n",
    "        clf = Classifier.load_from_checkpoint(ckpt_dir, symps=['uncertain'])\n",
    "        clf.eval()\n",
    "        clf.cuda()\n",
    "        \n",
    "        with xopen.xopen(input_dir) as fi:\n",
    "            for i, line in tqdm(enumerate(fi)):\n",
    "                record = json.loads(line)\n",
    "                user_sents = []\n",
    "                sent_bounds = [0]\n",
    "                curr_sid = 0\n",
    "                post_subj = []\n",
    "                for post in record[\"text\"]:\n",
    "                    post_subj.append(decide_subject(post))\n",
    "                    sents = cut_sentences(post, senttokenizer, nlp)\n",
    "                    curr_sid += len(sents)\n",
    "                    sent_bounds.append(curr_sid)\n",
    "                    user_sents.extend(sents)\n",
    "\n",
    "                all_probs = []\n",
    "                for i in range(0, len(user_sents), batch_size):\n",
    "                    curr_texts = user_sents[i:i+batch_size]\n",
    "                    processed_batch = infer_preprocess(curr_texts, tokenizer, max_len)\n",
    "                    for k, v in processed_batch.items():\n",
    "                        processed_batch[k] = v.to(clf.device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = clf(processed_batch)\n",
    "                        probs = logits.sigmoid().detach().cpu().numpy()\n",
    "                    all_probs.append(probs)\n",
    "                all_probs = np.concatenate(all_probs, 0)\n",
    "\n",
    "                # merge all sentence features into post-level feature by max pooling\n",
    "                all_post_probs = []\n",
    "                for i in range(len(sent_bounds)-1):\n",
    "                    lbound, rbound = sent_bounds[i], sent_bounds[i+1]\n",
    "                    post_prob = all_probs[lbound:rbound, 0].max()\n",
    "                    all_post_probs.append(post_prob)\n",
    "                all_post_probs = np.array(all_post_probs)\n",
    "                post_subj = np.array(post_subj)\n",
    "                data = {\n",
    "                    \"uncertain\": all_post_probs,\n",
    "                    \"subj\": post_subj\n",
    "                }\n",
    "                datastore.append(data)\n",
    "\n",
    "\n",
    "    df = pd.read_json(\"../../../data/postdatalines.json\", lines=True)\n",
    "    vector_df = pd.DataFrame(datastore)\n",
    "    df = pd.concat([df,vector_df],axis=1)\n",
    "    df.to_json(f\"../../../data/reweighting/uncertaintySubject{senttokenizer}.json\",lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OZP-compatibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
