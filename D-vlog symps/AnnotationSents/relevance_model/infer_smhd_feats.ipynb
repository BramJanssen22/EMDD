{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import yaml\n",
    "import xopen\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from os.path import dirname\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from data import infer_preprocess\n",
    "from model import Classifier, BERTDiseaseClassifier\n",
    "from utils import default_symps\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import blingfire\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentences(text, tokenizer, nlp):\n",
    "    if tokenizer == 'blingfire':\n",
    "        sents = blingfire.text_to_sentences(text.strip()).split(\"\\n\")\n",
    "    if tokenizer == 'nltk':\n",
    "        sents = sent_tokenize(text.strip())\n",
    "    if tokenizer == 'spacysm':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if tokenizer == 'spacylg':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    if tokenizer == 'spacytrf':\n",
    "        doc = nlp(text)\n",
    "        sents = [sent.text.strip() for sent in doc.sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         True\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.5\n",
      "\"exp_name\":           mbert_label_enhance_bal_sample_050_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data_w_control\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          exclude\n",
      "\"write_result_dir\":   ./lightning_logs/bal_sample_records.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "797it [04:17,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "datastore = []\n",
    "# options : blingfire, nltk, spacysm, spacylg, spacytrf\n",
    "senttokenizer = 'spacysm'\n",
    "# set spacy tokenizer\n",
    "if senttokenizer == 'spacysm':\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "if senttokenizer == 'spacylg':\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "if senttokenizer == 'spacytrf':\n",
    "    nlp = spacy.load(\"en_core_web_trf\")    \n",
    "\n",
    "symps = [\"Anxious_Mood\",\"Autonomic_symptoms\",\"Cardiovascular_symptoms\",\"Catatonic_behavior\",\"Decreased_energy_tiredness_fatigue\",\"Depressed_Mood\",\"Gastrointestinal_symptoms\",\"Genitourinary_symptoms\",\"Hyperactivity_agitation\",\"Impulsivity\",\"Inattention\",\"Indecisiveness\",\"Respiratory_symptoms\",\"Suicidal_ideas\",\"Worthlessness_and_guilty\",\"avoidance_of_stimuli\",\"compensatory_behaviors_to_prevent_weight_gain\",\"compulsions\",\"diminished_emotional_expression\",\"do_things_easily_get_painful_consequences\",\"drastical_shift_in_mood_and_energy\",\"fear_about_social_situations\",\"fear_of_gaining_weight\",\"fears_of_being_negatively_evaluated\",\"flight_of_ideas\",\"intrusion_symptoms\",\"loss_of_interest_or_motivation\",\"more_talktive\",\"obsession\",\"panic_fear\",\"pessimism\",\"poor_memory\",\"sleep_disturbance\",\"somatic_muscle\",\"somatic_symptoms_others\",\"somatic_symptoms_sensory\",\"weight_and_appetite_change\",\"Anger_Irritability\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 64\n",
    "    input_dir = \"../../../data/postdatalines.json\"\n",
    "    ckpt_dir = \"lightning_logs/version_0/checkpoints/epoch=0-step=720.ckpt\"\n",
    "    hparams_dir = os.path.join(dirname(dirname(ckpt_dir)), 'hparams.yaml')\n",
    "    hparams = yaml.load(open(hparams_dir),Loader=yaml.Loader)\n",
    "    max_len = hparams[\"max_len\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hparams[\"model_type\"])\n",
    "    clf = Classifier.load_from_checkpoint(ckpt_dir, symps=default_symps)\n",
    "    clf.eval()\n",
    "    clf.cuda()\n",
    "\n",
    "    allPostSentences = []\n",
    "    \n",
    "    with xopen.xopen(input_dir) as fi:\n",
    "        for i, line in tqdm(enumerate(fi)):\n",
    "            record = json.loads(line)\n",
    "            user_sents = []\n",
    "            sent_bounds = [0]\n",
    "            curr_sid = 0\n",
    "            if record['text'] == None:\n",
    "                break\n",
    "            else:\n",
    "                for post in record[\"text\"]:\n",
    "                    sents = cut_sentences(post, senttokenizer, nlp)\n",
    "                    curr_sid += len(sents)\n",
    "                    sent_bounds.append(curr_sid)\n",
    "                    user_sents.extend(sents)\n",
    "                all_probs = []\n",
    "                all_feats = []\n",
    "                for i in range(0, len(user_sents), batch_size):\n",
    "                    curr_texts = user_sents[i:i+batch_size]\n",
    "                    processed_batch = infer_preprocess(curr_texts, tokenizer, max_len)\n",
    "                    for k, v in processed_batch.items():\n",
    "                        processed_batch[k] = v.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        feats, logits = clf.feat_extract_avg(processed_batch)\n",
    "                        feats = feats.detach().cpu().numpy()\n",
    "                        probs = logits.sigmoid().detach().cpu().numpy()\n",
    "                    sentData = {\"sentence\": curr_texts, \"probabilities\": dict(zip(symps, probs))}\n",
    "                    allPostSentences.append(sentData)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "# df = pd.read_json(\"../../../data/postdatalines.json\", lines=True)\n",
    "# vector_df = pd.DataFrame(datastore)\n",
    "# df = pd.concat([df,vector_df],axis=1)\n",
    "# df.to_json(f\"../../../data/vectorData/Test{senttokenizer}Vectors.json\",lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "[7.9607414e-03 3.0320024e-04 1.0407658e-04 9.0106642e-03 1.0153279e-02\n",
      " 3.4104589e-02 2.4430244e-04 1.5734080e-03 7.5825855e-02 2.9496774e-02\n",
      " 4.8576433e-02 4.5706523e-03 3.1745611e-04 5.6496076e-03 8.3765751e-03\n",
      " 2.6174184e-02 4.9148109e-03 6.5465295e-04 2.6951968e-03 4.7332618e-02\n",
      " 7.9219081e-03 8.9806043e-02 2.1092191e-03 1.6790779e-02 1.7521903e-01\n",
      " 5.5154562e-03 1.7779199e-02 4.0076017e-01 7.7681732e-04 5.2708038e-04\n",
      " 4.8485263e-03 4.0203603e-03 9.6228626e-03 3.5640804e-04 3.9608913e-04\n",
      " 3.9182062e-04 2.8982684e-03 1.0988766e-02]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(len(allPostSentences[i]['sentence']))\n",
    "print(allPostSentences[i]['probabilities'][\"Anxious_Mood\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OZP-compatibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
