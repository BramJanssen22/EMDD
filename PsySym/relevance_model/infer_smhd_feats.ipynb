{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import yaml\n",
    "import xopen\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from os.path import dirname\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from data import infer_preprocess, cut_sentences\n",
    "from model import Classifier, BERTDiseaseClassifier\n",
    "from utils import default_symps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accumulation\":       1\n",
      "\"bal_sample\":         True\n",
      "\"bs\":                 64\n",
      "\"control_ratio\":      0.5\n",
      "\"exp_name\":           mbert_label_enhance_bal_sample_050_666\n",
      "\"gradient_clip_val\":  0.1\n",
      "\"input_dir\":          ../data/symp_data_w_control\n",
      "\"loss_mask\":          True\n",
      "\"loss_type\":          bce\n",
      "\"loss_weighting\":     mean\n",
      "\"lr\":                 0.0003\n",
      "\"max_len\":            64\n",
      "\"model_type\":         mental/mental-bert-base-uncased\n",
      "\"patience\":           4\n",
      "\"pos_weight_setting\": default\n",
      "\"seed\":               666\n",
      "\"threshold\":          0.5\n",
      "\"uncertain\":          exclude\n",
      "\"write_result_dir\":   ./lightning_logs/bal_sample_records.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_size = 64\n",
    "    patient_dir = \"../data/datastoreOZP/dvlog_wtext.json\"\n",
    "    output_dir = \"../../Relevance_inference_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    ckpt_dir = \"lightning_logs/version_0/checkpoints/epoch=0-step=720.ckpt\"\n",
    "    hparams_dir = os.path.join(dirname(dirname(ckpt_dir)), 'hparams.yaml')\n",
    "    hparams = yaml.load(open(hparams_dir),Loader=yaml.Loader)\n",
    "    max_len = hparams[\"max_len\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hparams[\"model_type\"])\n",
    "    clf = Classifier.load_from_checkpoint(ckpt_dir, symps=default_symps)\n",
    "    clf.eval()\n",
    "    clf.cuda()\n",
    "    split2dataset = []\n",
    "    \n",
    "    with xopen.xopen(patient_dir) as fi:\n",
    "        for i, line in enumerate(fi):\n",
    "            record = json.loads(line)\n",
    "            aid = \"P\" + str(record['id'])\n",
    "            user_sents = []\n",
    "            sent_bounds = [0]\n",
    "            curr_sid = 0\n",
    "            if record['posts'] == None:\n",
    "                break\n",
    "            else:\n",
    "                for post in record[\"posts\"]:\n",
    "                    sents = cut_sentences(post)\n",
    "                    curr_sid += len(sents)\n",
    "                    sent_bounds.append(curr_sid)\n",
    "                    user_sents.extend(sents)\n",
    "                all_probs = []\n",
    "                all_feats = []\n",
    "                for i in range(0, len(user_sents), batch_size):\n",
    "                    curr_texts = user_sents[i:i+batch_size]\n",
    "                    processed_batch = infer_preprocess(curr_texts, tokenizer, max_len)\n",
    "                    for k, v in processed_batch.items():\n",
    "                        processed_batch[k] = v.cuda()\n",
    "                    with torch.no_grad():\n",
    "                        feats, logits = clf.feat_extract_avg(processed_batch)\n",
    "                        feats = feats.detach().cpu().numpy()\n",
    "                        probs = logits.sigmoid().detach().cpu().numpy()\n",
    "                    all_probs.append(probs)\n",
    "                    all_feats.append(feats)\n",
    "                all_probs = np.concatenate(all_probs, 0)\n",
    "                all_feats = np.concatenate(all_feats, 0)\n",
    "\n",
    "                # merge all sentence features into post-level feature by max pooling\n",
    "                all_post_probs = []\n",
    "                all_post_feats = []\n",
    "                for i in range(len(sent_bounds)-1):\n",
    "                    lbound, rbound = sent_bounds[i], sent_bounds[i+1]\n",
    "                    post_prob = all_probs[lbound:rbound, :].max(0)\n",
    "                    all_post_probs.append(post_prob)\n",
    "                    post_feat = all_feats[lbound:rbound, :].mean(0)\n",
    "                    all_post_feats.append(post_feat)\n",
    "                all_post_probs = np.stack(all_post_probs, 0)\n",
    "                all_post_feats = np.stack(all_post_feats, 0)\n",
    "                data = {\n",
    "                    \"id\": aid,\n",
    "                    \"diseases\": record[\"diseases\"],\n",
    "                    \"probs\": all_post_probs,\n",
    "                    \"feats\": all_post_feats\n",
    "                }\n",
    "                split2dataset.append(data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(split2dataset)\n",
    "\n",
    "# print(\"Writing\")\n",
    "# for split, dataset in split2dataset.items():\n",
    "#     with open(os.path.join(output_dir, f\"{split}.pkl\"), \"wb\") as fo:   \n",
    "#         pickle.dump(dataset, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/datastoreOZP/Relevance_inference_data.csv')\n",
    "df.to_json('../data/datastoreOZP/Relevance_inference_data.json', orient='records',lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OZP-compatibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
